---
title: "R on HPC"
author: "Mariana Montes, Ehsan Moravveji, Geert Jan Bex"
institution: "Vlaams Supercomputer Centrum"
date: today
format:
  revealjs:
    transiton: slide
    slide-number: true
    incremental: true
code-annotations: select
---

## Overview

- How to run R on HPC systems
  - Open OnDemand
  - Batch jobs
- Benchmarking: efficiency
- Improving performance
  - Profiling
  - Code optimization
  - Parallel computing


# Running R on HPC systems

## Open OnDemand

- Web-based platform for accessing HPC resources
  - RStudio
  - Jupyter Lab
  - Terminal
  - File browser
- Convenient for interactive development
- Very inefficient for computations

::: {.fragment}
Access via https://ondemand.hpc.kuleuven.be/
:::


## Batch jobs

Example "computation" script:

```{.r filename="hello_world.R"}
# Get the hostname
hostname <- Sys.info()["nodename"]

# Print the message
cat("Hello from", hostname, "!\n")
```

No suprises here


## Testing the script

Terminal:

```bash
$ module load R/4.4.0-gfbf-2023a            # <1>
$ Rscript hello.R                           # <2>
Hello from login-0-0!
```

1. Load the R version you want to use
2. Run the script


## Module system


What & why?

- Different versions of software packages
- Reproducibility

::: {.fragment}
How?

- Search for modules: `module -r spider ^R/`
- Load a module: `module load R/4.4.0-gfbf-2023a`
- List loaded modules: `module list`
- Unload all modules: `module purge`
:::


## Job script

```{.bash filename="hello_world.slurm"}
#!/usr/bin/env -S bash -l                     # <1>
#SBATCH --account=lpt2_sysadmin               # <2>
#SBATCH --nodes=1                             # <3>
#SBATCH --ntasks=1                            # <4>
#SBATCH --cpus-per-task=1                     # <5>
#SBATCH --time=00:02:00                       # <6>
#SBATCH --cluster=wice                        # <7>

# first clean up your environment, then load R
module purge &> /dev/null                     # <8>
module load R/4.4.0-gfbf-2023a                # <9>

Rscript ./hello_world.R                       # <10>
```

1. Initialize the environment
2. Specify the account for credits
3. Number of nodes
4. Number of tasks
5. Number of CPUs per task
6. Maximum runtime of job
7. Cluster to run on
8. Unload all modules
9. Load the R version
10. Run the script


## Slurm speak

Slurm is

- resource manager
- job scheduler

::: {.fragment}
User(s) submit jobs to Slurm

- Job in queue
- When resources available, job starts
:::

::: {.fragment}
Efficient use of (very expensive) HPC resources
:::


## Submitting a job

```bash
$ sbatch hello_world.slurm                       # <1>
Submitted batch job 62136541 on cluster wice     # <2>
```

1. Submit the job script `hello_world.slurm`
2. Job ID is `62136541` (incremental)


## Checking job status

Show all your jobs on cluster wice:

```bash
$ squeue --cluster=wice
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          62136541      wice hello_wo  mmontes  R       0:00      1 p33c20n3
```

::: {.fragment}
`ST` is the job state:
- `PD`: pending
- `R`: running
- `CG`: completing
:::

::: {.fragment}
No jobs? All done!
:::


## Job output

Saved to file `slurm-<jobid>.out`:

```{.bash filename="slurm-62136541.out"}
SLURM_JOB_ID: 62136541
SLURM_JOB_USER: vsc30140
SLURM_JOB_ACCOUNT: lpt2_sysadmin
SLURM_JOB_NAME: hello_world.slurm
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: batch
SLURM_NNODES: 1
SLURM_NODELIST: p33c20n3
SLURM_JOB_CPUS_PER_NODE: 1
Date: Tue Aug  6 08:58:09 CEST 2024
Walltime: 00-00:02:00                              # <1>
========================================================================
Hello from p33c20n3 !                              # <2>
```

1. Info about job
2. Output of the script
